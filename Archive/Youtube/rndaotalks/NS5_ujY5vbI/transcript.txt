[ 0:00:16.000 ---> 0:00:54.000 ] Speaker A : So hello, everybody. Welcome to another arendelle talk, this time with Jamie Joyce from the society library. And we're talking deliberation decision making libraries of knowledge, so many things. And I wanted to take the opportunity to have Jamie explain their research process and how they do things at Society Library to come up with the outputs that they did, namely the paper and the map for the Diablo Canyon nuclear power plant debate. Let's call it in California. And Jamie. Take it away.
[ 0:00:55.000 ---> 0:14:49.000 ] Speaker B : Okay, sounds good. Hello, everyone. Just a quick show of hands, if you could. Who has seen our outputs, who has seen us give a presentation before and see the kinds of things that we create? Okay, so a lot of people here haven't seen that. Okay, great. So I'm going to start by showing some of these outputs so you know what I'm talking about. But I'll talk a little bit about the organization first. So I'm the executive director of a nonprofit called the Society Library. Our big, broad mission is to improve humanity's relationship to information, and that means building tools and products and services and providing knowledge work so that people are able to view complex issues through more comprehensive lenses. And so some of the things we do include actually, Andrea, you had an interest in decision making models. We create decision making models, political ones, at the city council level. We also have a toolkit. There's many ways in which you can make a decision, but we structured ours specifically for making a ballot decision. We've used our methodology to write legislation. We have done educational curricula at 32 different universities, mainly in the United States. But one of the most complex things that we do is build something that we call debate maps, which also serve as libraries. Essentially, a debate map is the ontological structure of the data in a library. So instead of the Dewey Decimal System and kind of organizing books by genre and author, the Society Library decided as we are collecting and organizing knowledge, we're going to go ahead and front load that into a debate map. So I will show you a little bit about what that looks like. And then for all the researchers out there, I'll tell you how we do it, even though I hope it will change very soon with increased implementation of AI. So I'm going to share my screen. Hopefully this works. Okay, can everyone see my screen? Lovely. So in general, Society Library, one of the big things that we do I'll just scroll down is, again, create all these artifacts. Currently, we're mapping debates. We want to move our political decision making models into more of, like, a digital congress, a public policy producing engine, and then inevitably just be this centralized knowledge base. So the Society Library is really like where we're headed. Even though it seems as though like we're creating these collections now, it's really just about creating debate maps now. And I'm going to skip. We've done a number of different subjects. Climate change COVID-19 election integrity. So many of these subjects are so comprehensive and broad, it takes immense amount of resources to complete. You'll notice these ones are broken up by subtopics, which are really big genres of a concept. When Diablo Canyon, which is the first one we've published, is 4000 points. So that essentially means claims, premises, arguments, et cetera. And when we create debate maps, what we're essentially doing is extracting arguments, claims and evidence from over twelve types of media to build databases that articulate their reasoning from all points of view. So let me show you what the database itself looks like and then I'll show you the front end. Here is our database. Some of you who saw a talk that I gave recently, I suspect, some of you, I think I know. That the talk you may have seen. I've shown this database before. We were given a grant to map out the debate about what should happen to the last remaining nuclear power plant in the state of California. This power plant has been the subject of protests for decades. People have been arguing about its economic feasibility, its safety, environmental issues, energy related issues, ethical issues, political issues for literally decades. And so what we did is essentially build this map. And it's funny that I'm starting from the question itself. I'll explain to you how we actually find the initial question. But if we're asking ourselves the question of what should happen to the Diablo Canyon nuclear power plant, instead of it being a keep it open, shut it down, which a lot of people may perceive as the two options, we found there's about nine different positions that people actually take. And how we come to these nine positions is by mass collecting data, deconstructing it down to the claim level, clustering it into categories, and essentially that aggregates up to a hierarchy. And I'll explain that process. I'm not sure if I'm going in the order correctly, but hopefully it'll all click together in the end. So if we start with just position one, we have a number of different there's so many types of arguments that are economic and environmental and related to safety and well being. So the position is this high level, kind of vague orientation towards the question. Position one is that the Diablo Canadian Nuclear Power Plants license should expire and it should be decommissioned as scheduled. A differentiation from that is that should be closed immediately and not as scheduled. It should be left open for 510 20 years only if regulations change. It should be turned into a hydrogen plan or desalination. There's all these different differentiations, which is essentially orientation, but with different caveats or conditions. But if we go back to position one and we see all these different categories of arguments, the reason why we break up things into categories is because there's just so many arguments that people make. So right here is economic. And these are all the economic highest level arguments. And then we have the same for environmental. So let me zoom in and give you an example. So when we're talking about high level arguments and claims, that means they're very vague. So here under the environmental section of arguments that support it should be closed down as scheduled. We have Diablo Canyon has a negative impact on marine life in the ocean ecosystem due to its cooling system. Now, if we think about the structure of that claim, those concepts are very broad and vague. And so at the Society Library, we then further deconstruct that argumentation to be more specific. So, for example, Diablo Canyon's went through cooling system causes water temperature to increase, which negatively impacts the ecosystem. A slight differentiation in terms of being more specific, but also needs to be broken down increasingly. So, like, okay, according to what evidence? We're still processing data. According to what evidence? By how many degrees? What kind of impacts are we talking about, et cetera. This is just economic work. This is environmental. This is safety unpacking here. This is energy related unpacking here, education and political. And these debate maps can go down extremely far. Like, I just want to skip over to one that's slightly unpacked, and you can kind of start seeing the types of data that we've got. We've got quotes and references and graphs. Things break down pretty extensively in some cases. There's an economic argument that I like to use because it's super easy to show the level of depth that we're talking about. Oh, here it is based on market forces. And the thing that we also do is break down arguments into each one of their respective premises. So if I just pack up this really quickly here we have one argument essentially stating that, all things considered, closing down the plant isn't going to have the kind of economic impact that we're thinking. Oh, no, this one is about reducing demand from customers. And this is all the different dimensions of making that economic argument of how and why demand for Diablo Canyon energy is going to lessen and the impact of that. And you can just literally continue breaking down this argument down and down and down and down. And so we call this debate mapping. It's essentially a combination of argument mapping, which is more rigorous in terms of its structure, but also including that in higher level deliberation, like the vague positions and things like that. So who wants to look at a database like this? I'm not sure. I certainly do. But we were told, you can't just publish something like this. No one wants to look at this. So what we did was come up with something we call Society Library Papers. And essentially what this is is this is a briefing document that contains all of the information of the database itself. And what you can do is we can click on the word economic and that's going to summarize all of the high level arguments. So all of those nodes that we unpacked, that huge list of nodes, that's all here in narrative format, and you can click on one. We've got notes, right? So this is something really important. We have this sentence. There are market forces, financial incentives, policy decisions which have essentially made Diablocan redundant, non competitive, undesirable. We have a note here. This could now be outdated because conditions have changed since the claim was made. And we can unpack and explore that further. And we can see like, oh, there's draft legislation. PG and E may be given $1.4 billion. So it's going to be kind of moot that the market dynamics aren't favorable to this nuclear power plant because the federal government of the United States has an interest in providing stimulus to nuclear power plants with tons of funding if they are operating in unfavorable market conditions. So I mean these depending on the argument you click, you can just unpack further and further and further. We also have the ability where you can in the instance that we have claims that have various phrases, one of the things that we really want to do is be flexible in the expression of content. So if we go quickly back to the map itself, there's a whole bunch of information that we can pack into one node. So here we have a tag, right? This is contested. So that means that we're trying to signal to our readers when they're looking at the map, much like seeing this note, that something could be outdated, that this is a contested claim. There's con argumentation here, which means we don't want people to stop where they're unpacking. But there's a lot more information packed into this claim here. We've got definitions that are a part of it, the definition of the NRC. We've got media, which is this graph. We've got references that need to be that essentially link back. And then we've got also quotes. So this content right here is where this claim came from. And it could be either the exact language from which we derived the claim or it's just the clue. It's how we surface that this claim was being made. So anyway, one of the things that we also include is various versions of claims now that's unfortunately very time consuming. Human curators have to do this in the future. AI will do it. Yay. But what we have is essentially you can opt into different versions of the same claim. So if you go to the right, it's supposed to be a technical if we have it available. This one we do have a simple version for, which is according to PG E, there are three main reasons why will be less need for electricity from Diablo Canyon as opposed to this claim. According to PG and E, in 2016, three key trends have significantly reduced PG and E's electricity sales in recent years and will likely have an even greater impact on the future. The downward pressure on sales is reducing the need for electricity from Diablo Canyon. So just different versions of the same sentiment. You lose a little bit when you simplify things, you add a little bit when you are using Jargon and things like that, but it's mostly for accessibility. And this goes back to the mission of the Society Library being a nonprofit where we're trying to improve humanity's relationship to information. So for this product, it's really important that we're inclusive and we're as accessible as we can. Of course, all of that is very expensive. We hope to automate a lot of it in the future. So how do we create this unpackable piece of paper? Oh, and another feature is that you can essentially explore every single claim. There's like a little mini Wikipedia page on every claim. So here's the claim. Here's a description of the claim. Just to give you more context. The quote from which the claim was derived, it being more simply put oh, we do have a more technically put nice as said on the internet. So this is like a quote just from like a tweet or something like that. Here are the references here's. Relationship to other claims. Socilibra is pretty small and scrappy. This is like our first push out of this kind of front end. But I'm going to pause and see if I've made sense at all or if anyone has any questions before I talk about how do we produce these ridiculous data sets of structured knowledge like this. So let me pause and see if anyone has questions. Yes, Andrea, so often points will support one argument and undermine a different one. How does that handled in the data structure? Good question. So I believe we've recently just resolved the infinite recursion problem. What we did before is essentially there's various ways in which we can clone nodes. So this looks like a tree structure, it's actually a graph database and we have different things that we can do. So we can clone nodes and we can copy nodes and cut and paste nodes. So the cloning feature is essentially like reproducing the node while being able to edit it. But at the same time we have a differentiation of our cloning capability where essentially we're creating a differentiated clone that if you edit it, it's not going to edit all the copies from which it was copied from. Our copy feature does that where you can essentially create copies and they're all the same throughout. So in order to overcome the infinite recursion, we use clone which clones the top node, makes it so it's not connected to all the other copies, but all of the children nodes are connected to all of the other copies. So if that's updated and changed, it's just linked. But that means that the other node that we copy is not going to recognize that this clone is a copy. So it's not just going to infinitely unpack forever. It's hard to explain, but essentially we have accommodated for that in the data structure. It's just like how we go about connecting the notes. It's like a two step process and we have a special way of making a copy of it. Yeah. So it just doesn't go infinitely back and forth. Any other questions before I move on? Yes, Paolo?
[ 0:14:50.000 ---> 0:15:07.000 ] Speaker A : Yeah, just maybe one about zooming out a little bit. Why is this needed? Right? A little bit of context on all of this and why the society libraries investing so much effort in doing this?
[ 0:15:07.000 ---> 0:19:06.000 ] Speaker B : Super great question. So I gave a talk about this yesterday, is that I don't think that unless you're a knowledge worker, you really see how bad the epistemic environment really is. I think there may have been an opportunity in the past, whether it was just a dream that people had about the nature of the web or it lies only in the future. But a linked knowledge web is something that I think people could really benefit from. Chain of providence, evidence linked to claims. These sorts of things could have been like the base upon which we built the web, but we don't our web is broken. There's so much redundancy of knowledge, there's so much like broken chains of providence. I gave a talk yesterday, or actually my colleague gave a talk alongside me, and she talked about this one example from Diablo Canyon where there's this claim that's been made so often, about 1.5 billion fish being sucked up into its OTC system. Getting back to that marine life claim that I showed earlier, 1.5 billion fish sucked up into the OTC system, and we have found it in newspaper articles, on social media, activist websites, in policy, never linked to any evidence. We saw it so often, but never linked to any actual evidence. I began to think personally, it was like a weird myth. Like, is this just like a cultural myth that's persisted for like a decade? And then we finally, after going through like, 15 agencies worth of government documents, found the 478 page study where it explained, this is how we came up to the number of 1.48 billion fish being entrained in the OTC system. Yada. Yada. And so unless someone's willing to do an immense amount of work, they may be interacting with a very superficial layer of information. And just because of where it comes from, because it's said with such conviction from an activist or it's in a policy document or a newspaper is reporting it, we assume it must be veritable. In this case, it was. But the fact that we do not have this linked knowledge just means that there's so much more surface area for us to be fooled and propagandized or misled, whether it's intentional or not intentional. So the society Library dreams of there being some kind of commonly held societal infrastructure of linked knowledge, so that people can explore all different points of view and they can actually see what evidence connects to what arguments, what kind of arguments exist so that we can make more rational and informed decisions about these sorts of things. Because I think we are deceived by how readily available information is. When I search something on Google, something comes up. And so I think a lot of people have the impression that this works great. Wow. My tools work. I get the information I'm looking for without realizing that there's many layers of depth that are often missing because the web is broken and knowledge is not linked. So a Society Library is doing this manually, mostly just to prove like it can be done, because there's been like 30 years of people trying to do this and failing. And I can actually talk about since this whole gathering of individuals is about talking about research methods. One of the things the Society library did before we developed our research methods is we looked at 168 different systems, and we saw, okay, a lot of them are making the same assumptions about human behavior, about tool usage and these sorts of things. And maybe that's some of the reason why they failed never really took off, only stays in academia. So we specifically curated our strategy to avoid what we saw as some of the perceived pitfalls of so many other projects that wanted to create linked knowledge libraries at the argument and claim level, because there's plenty of linked knowledge libraries. Libraries themselves are linked knowledge, but at the argument and claim level. So that's why I think this is important, because I think it is the inevitable evolution of where information in a digital age should go in terms of its own development and someone should be doing it. So we are.
[ 0:19:08.000 ---> 0:19:09.000 ] Speaker A : Thank you.
[ 0:19:10.000 ---> 0:19:15.000 ] Speaker B : Any other questions before I move on? I'm sorry.
[ 0:19:15.000 ---> 0:19:28.000 ] Speaker C : Yes, I have a question. Is this accessible to all or you have to it's a service, right? That everybody can just be part by it or I'm not sure how that works.
[ 0:19:29.000 ---> 0:20:48.000 ] Speaker B : Yeah, I'm still sharing my screen. I just pulled Tabs out. Yes, the Society Library is a 501 nonprofit organization, and we give away this data for free. We're trying to get people to imagine what a new digital public library could be. So instead of going to a physical location and renting a book or going and downloading a book from a digital bookshelf from a digital library, the Society Library is about creating a library of society's ideas. And so we've decided to kind of target the niche that Wikipedia doesn't do well. Wikipedia is a fantastic resource, but when it comes to politicized, the Wikipedia has their standards about what they will include in terms of information, and they make decisions about what to prioritize. And the Society Library is, you know, we're not trying to confine something to a digital encyclopedia page like Wikipedia is. We want to create libraries. So we have this Diablo Canyon library where you can see all the different books from all the different authors, from all the different genres. What that really means is that you're seeing all of the clean, consolidated, steel manned, fact checked arguments, claims, and evidence from all points of view and across economic safety, blah, blah, blah dimensions that we find people really care about and are talking about. And our goal is just we'll save you 10,000 hours of research if you want to learn about this subject.
[ 0:20:50.000 ---> 0:21:10.000 ] Speaker C : I think something that this is amazing. I even start to think, like, how this going to be also proof of actually correlation between stuff and yeah. Oh, wow, I have to get into this one. I love.
[ 0:21:12.000 ---> 0:22:00.000 ] Speaker D : So, Jamie, jamie, I have a question for you. I can see the value of it. It's amazing. It really is phenomenal. And I'm curious about what the strategy is for making this accessible to a larger population of people and then how you're staying funded. This is a labor intense job, right? Without question. Now, AI will shorten that labor, but it still requires the human element to go into the discipline of inquiry as deep as you're going. So how is it funded? And then how do you expand it out to a large population? Like, would a new search engine like Tempest be a good partner for this?
[ 0:22:00.000 ---> 0:24:06.000 ] Speaker B : Okay, so great series of questions. I'll mention the funding thing. So the Society Library is deceptively a tiny nonprofit. A lot of people think we're very large, and we are not. This entire Diablo Canyon project, including the creation of the paper's front end, was done with a grant of $78,000. So we're extraordinarily cash efficient, and we hire librarian, analysts who are incredibly talented, incredibly well educated, but they don't get data scientist salaries, even though they are totally underrated data scientists. I'd obviously love to give them as much as a data scientist, but we're just like a group of super passionate scrappy analysts and researchers. And we get a lot of volunteer contribution, too, in terms of tech time and developer time. So often a lot of our funding comes in through private donors. So just like one family fund or one DAF or a donor here and there, dropping 10,000 here or there or even a couple of $100, we just kind of, like, pulled that together. And that's why when I was showing the presentation, that there was many subjects that we tackled, but there's too expensive for us to have finished. We started COVID-19 found, I think, 576 different dimensions of the debate, 274 for climate change. So we've started the scaffolding of these. But to actually execute is very expensive, and we're like a tiny nonprofit. We just broke the $100,000 a year annual budget this year, which is pretty nuts. So people we did recently just finally get institutional funding. The International Fact Checking Network just gave us a grant to teach our methods to fact checkers. So we're actually building an educational curricula for the world's fact checkers to kind of teach them about logical argumentation and steel manning. And not so much just like labeling something as true or not true based on the presence of evidence or not, but to be a little bit more nuanced about these things. So we're starting to get prestige and recognition, I think that'll only continue. And then the second part of your question was what?
[ 0:24:10.000 ---> 0:24:27.000 ] Speaker D : There was three parts, I guess one part was how do we get it up to a larger population people to look at it? And then the last part is who is a good partner that actually can accelerate it? Like, who can you jump in the apple cart with, so to speak?
[ 0:24:27.000 ---> 0:27:45.000 ] Speaker B : Thank you for reminding me. This is why collective intelligence is so important. Okay, so the strategy currently for the Society Library is not to invest in building our own audience. Would it be lovely if we just magically got an audience by going viral or something like that? Of course. But we're a very complex thing for people to understand because we're metaphorically taking things like libraries and debates and transforming them with technology. And that can be difficult for people to wrap their head around. And that's like the closest language to describing what we do besides Jargon. And so I can't imagine we're going to have an audience for some time. So our strategy is work with the people who already have audiences and improve their knowledge output and then you improve the epistemic commons. So for example, there's a number of debate organizations who they have consistent sustainable funding because they're able to prove that the way in which they conduct debates for the public depolarizes attitudes, which is really big right now. So we've approached a number of those organizations and said, hey, awesome that you're bringing mediation techniques to debate, or that you give it an air of dignity so that people feel called to be on their best behavior and are following rules and that changes people's minds and blah, blah, blah. But that doesn't necessarily mean that the arguments that are being exchanged in the format that you're carefully curating are less BS. So let us do the work of mapping out deliberations. And we've done this with organizations. We've gone to debate organizations and said, we took a fact that you had in your brief as a summary of what was discussed here's a mini debate map about that specific claim being made. And we can just really upgrade your output in terms of what you push out to people or in real time. You can be checking the database to see, okay, someone made a claim. What are all the different counterclaims to that and just being more comprehensive in how those things are conducted. You mentioned a search engine. We are super interested in partnering with search engines because that's another outlet through which audiences and users already exist, where we could just funnel data through. So there's the debate mapping organizations. There's also journalists. We've been reaching out to journalists and saying, hey, journalist, we saw you made this claim. You linked to no evidence. Here's a page of evidence and procon argumentation about that. Why don't you just link to this? We're thinking that maybe what we would have to do is give the data away. They would generate a page on their own website so they can get the ad revenue. And it's like, whatever, we're a nonprofit. We want to improve the epistemic space. We can give them the content. They put it in a format that's familiar, they link it to themselves. Everyone's better off. But also search engines. So Google, for example, pulls in data from Wikipedia enterprise. So Wikipedia sells their data as enterprise access to the likes of Google and other companies. When you Google something, if Wikipedia has data on that, oftentimes Google will surface that at the top of its search result. Sometimes that's actually a Google page. I'm writing an article right now about how you can mess with that. I was like, just changing Wikipedia, and then the next day, the search engine would be like the opposite, like an untrue thing. But Google auto generated its own page, like Googleartsandculture.com, and just like the thing that it's like, you can mess with these kinds of things. And so after I make an article about that, it's actually how we got the fact checking grant, is we mess with fact checkers and then showed them we can prove it.
[ 0:27:45.000 ---> 0:27:58.000 ] Speaker D : So, unfortunately, I can't stay on much longer, but I just put my contact information in the chat. Please reach out to me. I want to make an introduction to a search engine that I think you need to get on board with.
[ 0:27:58.000 ---> 0:28:00.000 ] Speaker B : We would love that, Larry. Thank you so much.
[ 0:28:00.000 ---> 0:28:04.000 ] Speaker D : All right, thank you. Thanks for what you're it's very cool. Very cool work.
[ 0:28:05.000 ---> 0:28:53.000 ] Speaker B : Thank you. Okay, great. So I'm going to save that and then we can go on to talking about methods, if you all are interested. How do we create these debate maps? There's hundreds of different debate mapping platforms out there. They vary in their level of robustness. There's some that are really popular, like Chialo, but the logical structure of Kealo's interface is not very sophisticated in terms of the procon relationships. They just have pros and cons underneath a parent. So how did we find this system that can accommodate enough complexity, or at least so we think, without being too academic and overwhelming? And how do we get people to actually produce this? So let me pull up my screen again and we'll dive into that.
[ 0:28:55.000 ---> 0:28:58.000 ] Speaker C : Quick question before you continue, if you could, please.
[ 0:28:58.000 ---> 0:28:59.000 ] Speaker B : Of course.
[ 0:28:59.000 ---> 0:29:05.000 ] Speaker C : How do you verify the accuracy. Of the information that it enters in the database.
[ 0:29:05.000 ---> 0:31:57.000 ] Speaker B : Well, there's various different ways in which we assess accuracy. So it's like, is it accuracy of the meaning of the thing itself or is it accuracy of our representation of it? Is it accuracy of how we've input it? There's a number of different things that we have to look at. One thing in terms of in the discussion of meaning is that the Society library is never going to tell anyone what's true or not true. We are not fact checkers. We're not going to say this is true and this is false. All we do is contextualize by argumentation. So if we input something in the database that's not frivolous, right? Like, if we can actually make a standard form claim out of it, and it's not like just insane speak that we can't make sense of and therefore omit, because we literally don't know what it means. If it's not frivolous and we input it, especially if it's popular and relevant, salient all these things, we would do the work of doing what we call devil's advocacy. So we do the work of, all right, let's steal Manet, let's see what evidence supports this, what arguments support this, let's see if it's found anywhere substantial like we did with the 1.5 billion fish. We spent way too much time eventually finding evidence for this 1.5 billion fish, and it was consequential. But we also do the opposite. And this will get into some of the methods. We've got 22 different methods across, whether it's a cultural method or it's like a long term method, knowledge policies like actual research techniques, 22 different methods for overcoming our own biases. So one of those is devil's advocacy. And when we do have devil's advocacy, it's counterargumentation. And so that's really what we do. We're not going to tell anyone something's accurate or not, but we're going to reveal to people by doing the work of Steel Manning and doing devil's advocacy, arguing for and against to show via context. And that's why we have those notes. Something like contested. It's like, we want you to see this is contested. So don't stop in the map here, don't stop in the papers here. You need to unpack further because there's counterargumentation to this. We don't want you to miss it. And then it's up to people. And really, when someone decides in their own mind that something is true, it's a heuristic of their own. Some people are going to see graphs or evidence, or they're going to look into how substantial a study was, or that it was a meta analysis was performed, or it's something that's just generally accepted in a field and say, I put a lot of value in that, I'm going to consider this to be accurate. And then some people are like, oh, there's a piece of content from the Bible that sounds like this, that's true to me because the word of God is truth. Right? So we'll never tell anyone what's true, not true. We just focus on making sure that we're comprehensive, we're inclusive, and that we just have all the right signals, the visual signals. And again, we're a tiny nonprofit, so we only have so big of a design budget. We're like doing the best we can, hopefully to do better in the future. The design elements to signal to people, to have that comprehensive view of a section of a relationship between nodes in order to get a more clear picture of its accuracy or inaccuracy.
[ 0:31:58.000 ---> 0:31:59.000 ] Speaker C : Basically evidence. Right?
[ 0:31:59.000 ---> 0:32:03.000 ] Speaker B : Like evidence, evidence connected.
[ 0:32:03.000 ---> 0:32:06.000 ] Speaker C : And then you kind of decide whether you want to agree with it or not, right?
[ 0:32:06.000 ---> 0:32:17.000 ] Speaker B : Yes. It's up to the individual at the end of the day. And a lot of people don't like that. But I think it's just like practically it's just like the practical way in which we all experience the idea of truth.
[ 0:32:17.000 ---> 0:32:29.000 ] Speaker C : Oh, I love it because we don't think it's the same. And now we have all these claims everywhere, this and that. But who made that decision?
[ 0:32:29.000 ---> 0:32:38.000 ] Speaker B : That's another thing, right? Yeah. So the society library's decision is that we're not going to make the decision for you, it's up to you, but we're just saving you 10,000 hours of finding all this stuff.
[ 0:32:39.000 ---> 0:32:42.000 ] Speaker C : This is genius. Genius, genius.
[ 0:32:44.000 ---> 0:33:03.000 ] Speaker B : Thank you. Okay, any other questions before I move on to the process? Okay, let's talk about tools and stuff. Yay. Can you see my screen? I can't hear anybody. Or can someone shout out, can you see?
[ 0:33:03.000 ---> 0:33:04.000 ] Speaker A : Yes.
[ 0:33:04.000 ---> 0:33:58.000 ] Speaker B : Lovely, thank you. Okay, so this is the general overview of our process, which is essentially that we start with some archival methods. We transcribe that into like machine readable text. We begin extracting and categorizing claims and arguments. We start inputting in the database and then we focus on visualizing and tagging and all that stuff. So when we archive something, there's a number of different tools. The first step that we do is something we call a topic flyover. So that means we're going to start zeroing in on who are the most popular thought leaders in this space, what's the most viral content, and just start essentially scraping together what are the high level categories of this. So a topic catalog. I think I may actually have to present this. One moment. Okay, let's see. Can you still see my screen?
[ 0:33:59.000 ---> 0:34:00.000 ] Speaker A : Confirmed.
[ 0:34:01.000 ---> 0:34:19.000 ] Speaker B : Yay. Okay, great. So if I click on topic catalog, these are like subtopics that we started picking up on immediately. So, like, what is SARS Cove two? A naturally occurring zoonotic coronavirus? A man made or known virus? A new coronavirus developed as a bioweapon? Yada. Yada. Yada.
[ 0:34:19.000 ---> 0:34:19.000 ] Speaker A : Yada.
[ 0:34:20.000 ---> 0:48:50.000 ] Speaker B : So we go on and on. So we just essentially take a look at the media and popular content for COVID. It was such a new thing, so it was just you could kind of grab it from anywhere. And this is what we call a topic flyover. So essentially it gives us the place to start. And then once we have all of these different topics, we create something that we call an archive catalog. Let me click on that. And you're seeing my screen change, is that right? Yes, we're following. Okay, great. Yeah, and let me see if I can get rid of Yay. So then we start looking at this is not a particularly great example. We start selectively targeting different article like different type media types where this claim could live. So we're talking about justifying the use of the vaccine. And this is very old data. Again, I think we had like a max $10,000 to work on COVID. But we have news and websites, scholarly articles, books and textbooks. We start identifying experts, we start identifying videos. And so basically what we're doing is we do this flyover, we get the high level topics. Then for every single topic, we create an archive catalog where we find different forms of media that have this sentiment in it. And then we essentially do that unpacking process down and down and down and down. Because as we completely deconstruct as you'll see, in a moment, as we deconstruct a media artifact that contains one topic, it's going to reveal to us 20 other topics that are referenced in that media artifact. And then we start over. We zero in on that new subtopic, we find a bunch of different targeted media artifacts that contain that subtopic and then unpack all of those and find even more subtopics. So it's just like this constant hierarchical breaking down and discovery process. And there's a variety of different things we do to make sure that we're comprehensive. Because like I said, the Society Library really cares about being really inclusive, really rigorous, really comprehensive in this exploration. So when we're creating the skeleton framework of the topic space, that's kind of like the method of how we begin that. But there's a number of things that we use to make sure that we're doing it to combat our own biases. So one thing is that we have custom search engines. We've created a number of different curated search engines that target certain areas of the web. So for example, we've got these databases of news article like outlets that we've broken down across a political spectrum. It does not matter if we are accurate in how we've labeled these news outlets. The important thing is that we're checking all of them just to see what nuances and variations on the expression of this sentiment are being expressed across the political spectrum. We can also create curated feeds from different websites, like government websites and things like that. We'll do recorded interviews. I just want to emphasize this. Every single time we've ever worked on a subject, no matter how exhaustively, how much data we've gathered about a debate, we always get new novel stuff when we interview people. So talking to people is very important because there is a digital divide. A lot of people don't put things into digital print, so to speak. So it's very important to talk to people, talk to stakeholders, send out surveys, solicit more information. We also have access to a number of databases, so we can search television, we can search the world's news through things like GDELT. Depending upon the subject, there can be all sorts of open and available data sets. So we gather and select those. And then what's most important is like tools and training. So we have our analysts, if they're new, go through this rigorous process of deconstructing content on the claim level, learn what we mean by a claim, teaching them to invert it to its opposite, on and on and on. So it's a combination of tools and methods and training that is step one of our process, which is just archiving great. And then once we have all that data, we've got some tools to transcribe it and turn into machine readable text. Yay, this is going to be so helpful when AI can substitute a lot of this work, which I'm very excited for one day. And then something that we do is extract and categorize. So again, this relies on a lot of training. The society library has standardized what it means by a claim, what it means by an argument, et cetera. We do have argument mining tools, so AI powered argument mining tools. Essentially that means argument detection within natural language. So we'll take the media artifacts that we gather and we'll port it through the argument mining AI. That will deliver arguments to us, but something that we also do. And I think I pulled up an example, my favorite example, Sean Hannity. This is an example of society library media deconstruction. So we were actually asked specifically to work on this by another nonprofit. This is a 17 minutes clip by Sean Hannity. And they said, this guy said so much in a 17 minutes segment, and I have no idea how to deal with it being true or not. And I think there's like 438 implied claims, 138 actual claims, something to that effect that he made. And it's just like to think about the human brain processing all of that and actually having the critical thought process of being like, is that true? Having that moment of skepticism is just like, I think it's impossible to think the human mind could really do that rigorously. So I think we're absorbing a lot of content. But anyway, so this is an example of taking a video, breaking it down into text. Here we've got the people who are speaking, the timestamp, the transcription itself. We have notes on imagery, notes on the video, notes on the music. Like there's clapping here. That's a gesture of approval that could have a subtle impact on how people are perceiving things. And then what our analysts do is they literally deconstruct it line by line. So they will take lines in the transcript of things that are actually being said and then they will standardize that to a more standard claim format. So here's like one sentence, line 17. And you can see we got a number of different claims from line 17. So this is a very compound statement. You get free higher education, you get free health care, free vacation, free green housing, free healthy food, universal basic income. And so we standardize that too because we know from an earlier line he's referencing the Green New Deal, we standardize it into those claims and then you have all of these different claims. And I mean we do do know screenshot deconstruction because again, the imagery does matter and YouTube comments as well. And this is all very lovely machine learning training data. But anyway, the purpose of deconstructing things down to the claim level like that is that you can begin to then cluster and categorize. It so great we've got all these Green New Deal claims and then because we've got 100 Green New Deal claims, we can see that they start breaking up and disambiguating into health care, into environmental stuff, into social justice. So we can begin clustering and then after a cluster, begin internally seeing the disambiguation of topics from within a cluster. And so we do that. And then here's an example, I kind of showed you an example already, but in this one statement, which is a natural language snippet we can have this many derived claims and this many implied claims. So we are really rigorous about pulling out as much meaning as possible from natural language for the purpose of fact checking every tiny little discrete implied claim within them. And implied claims just kind of means it's not something that can be derived from the natural language with certainty that that's what it implies to mean. But an implied claim means using the existing language. This is something that may be interpreted as implied in this statement. So essentially, sometimes these are like the unstated premises of an argument in order to unstated premises of an argument essentially that are missing that if were there, could create a valid or more sound argument. Now, one of the reasons why I got tripped up at the beginning of this talk about where to start is that the society library kind of like works backwards and then turns immediately around. So as you saw in the database right here, we start with a question right here and then we have positions, then we have categories and then we have high level arguments, then we break down those arguments into claims and evidence. But how we get to that hierarchy is actually the opposite. So we start with collecting the references and evidence, right, by doing that topic flyover, creating those little catalog sheets for every topic, finding little media artifacts that contain that topic. And then we extract the arguments and claims like you saw. We do the clustering into categories and subcategories. And then what that does is that it starts to indicate to us what positions exist. Why are people making these arguments? Like why are people bothering? It's usually because they're taking a stance. Now, what's important to understand about the Society library is that we deal with complex social and political issues. So they are debatable things, not every I mean, I would argue that technically all claims are debatable, but we take highly contentious, we go after the impactful, persistent polarizing issues. So that means people are taking positions and it's through this process of deconstructing down to the claim level and clustering up do we aggregate positions that people are taking on something? And then once we have positions, we have a cluster of positions which actually contain, as you can literally see here, these positions contain all of this, then we actually find out what the question is. So once you have a series of positions, you can see what they're taking a position on, which is the question. And what's really important about this and something that I think that we've also resolved is that if you arbitrarily ask a question that can lead to really messy argument mapping because you may not have strict bounds on the scope of relevance of the argumentation. Let me give an example of that. So in our climate change data, we have only six questions. We have 396,000 premises in the database, it's in a CSV file and we had to export it when we were building our new thing. But anyway, there's a lot of claims, there's like 278 subtopics that contain all those claims. But in all of those subtopics, all of them have positions which correspond to answering one of six questions. Now, extreme weather events are a topic within climate change. They're a category of content that could be expressed. There's plenty of arguments about extreme weather events. So if you just started mapping about extreme weather events without a clear question that defines the scope, you're going to be mapping a whole lot of things. And it's hard to explain to people who haven't done this work themselves, maybe some of you have, but essentially you lead into that recursive issue where something is kind of related to something else. So you link it together but you don't know the conclusion that you're arguing toward because you don't have a question that's demanding an answer. And so the question about like let's go back to extreme weather events. We have the topic of extreme weather events, that's relevant to two questions. One question is is climate change happening? The other question is what is the impact of climate change? And so when we are arguing about that, there is an increased frequency and intensity of extreme weather events that goes under evidence that climate change is happening. When we talk about the devastation and destruction to agriculture and human life and well being from extreme weather events that goes into under the question of what is the impact of climate change and they don't connect in society library ontology they are related but they do not connect because the questions themselves define what is relevant to talk about. And for the Society Library we're interested in answering questions, not again providing truth, but just mapping out all the different points of view and answering what we call the fundamental questions that society has in a specific domain. And we're creating this ontological structure. So we actually argue towards an answer and we just don't argue infinitely forever, which is something that I think if AI doesn't catch up on in terms of finding fundamental questions or orientations or understanding context and being able to compute context very deeply, then you're just going to be able to argue with an AI forever and ever and ever and ever. And you may not get the complete concrete answer that you're looking for if it doesn't understand deliberation and context as a part of its question answering process. So that's how we get to our hierarchical structure which is very nerdy but also very important to how the Society Library relates to our work. And then like I said, we map it. You've seen the Debate mapping software, there's all these different nodes, question nodes, category nodes, argument nodes, where we can break it up into logical proofs with premises and conclusions, claim nodes, definitions, various phrasing and then all of the relationships between nodes is like procon truth and procon relevance. We have node attachments, video, image media, equations, references, quotes, and there are impact in truth scores but we don't turn those on. Like one day in the future we may want to connect with models to start calculating or indicating to people some sort of confidence measure but that's a whole can of worms that we're not interested in getting involved in right now. But we understand that some people may not read all of this content and they may want to opt into a system. So having the capability to quantify certain dimensions of this is something that we just are keeping available for the meantime. And that was the idea of the person who created Debate Map who's been a part of the Society Labor for a long time. When I mentioned that we looked at 168 different systems, we actually found his and partnered with Hen to develop it for our purposes. And then yeah, we can present it, we can give tours of it. You saw what ended up being our latest interface, which is Sci Library papers. But yeah, that is kind of like the high level walkthrough. Something that's really important to know is that with any research methods you are building something towards an end. So we had the output that we wanted in mind and we build the tools in the pipeline to end up at that output. So it's kind of bespoke I don't know how useful it is to everybody else to adopt some of our methodologies or tools because we're trying to get to a very specific endpoint. That's generally how the site library does it. We have people working on models right now to automate different dimensions of the pipeline. And I think in the near future, especially with the way these language models are developing so quickly, I think we may be able to have real time, live, comprehensive libraries. And humans will just be in the loop to do integrity checks and do a stamp of, like, human looked at this node versus human has not yet looked at this node. Yeah. So does anyone have any questions now?
[ 0:48:52.000 ---> 0:48:56.000 ] Speaker C : I do, and I'll interrupt. Apollo can't jump in fast enough.
[ 0:48:57.000 ---> 0:49:40.000 ] Speaker B : How do you know? You come up with a good question. It kind of reveals itself in the data. And so we've had to revise questions many times. So it's like when we actually start mapping, it's like, is this clean? Like, are we actually headed towards a direction of conclusion or are we suddenly taking a turn and kind of bleeding into another question or going into a different topic? So it's literally the cleanliness of the data clicking into place, where it's like, okay, this is what we call the most fundamental question. It's not like it's a good question, it's just this is like the most base regressed back question that is so specific in scope of relevance next to all the other questions that it's going to make the data really clean to input. So your whole data structure ends up.
[ 0:49:40.000 ---> 0:49:42.000 ] Speaker C : Like, this is cohering around this rather.
[ 0:49:42.000 ---> 0:49:44.000 ] Speaker B : Than getting bleeding into other topics.
[ 0:49:44.000 ---> 0:49:44.000 ] Speaker A : Okay.
[ 0:49:44.000 ---> 0:49:58.000 ] Speaker B : Yeah, exactly. It's a big, complex way of answering question, but instead of answering it with, like a fact, it's answering it with, here's all the different points of view you could take on this and all the arguments why any one of these answers could be the answer you're looking for.
[ 0:49:59.000 ---> 0:50:00.000 ] Speaker A : Awesome, thanks.
[ 0:50:01.000 ---> 0:50:03.000 ] Speaker B : And then, Paolo? Yes?
[ 0:50:03.000 ---> 0:50:13.000 ] Speaker A : I have a question. By the way, did the initial question of the Diabol Canyon map was updated recently?
[ 0:50:14.000 ---> 0:50:18.000 ] Speaker B : I'm not sure I believe it's what should happen? So it's a should.
[ 0:50:20.000 ---> 0:50:38.000 ] Speaker A : I think I think I saw it some weeks ago and there was not the phrasing. That's why I'm asking. And I saw it and I was like, this is a bit maybe biased the way it was phrased before, because I think it was phrased before should the nuclear power plant be discontinued as planned or something like that.
[ 0:50:38.000 ---> 0:50:45.000 ] Speaker B : Oh, that's a position. So a position is I don't think it's changed recently, so maybe you're I don't know.
[ 0:50:45.000 ---> 0:51:00.000 ] Speaker A : I have an idea. I have an idea, but I had the suspicion that it changed because now when I read it in that phrasing, I was like, there was not maybe the phrasing that I had, which is good. And it proves kind of the point that you were making, which was that you have to do the mapping to come up with the right question.
[ 0:51:00.000 ---> 0:51:09.000 ] Speaker B : Right? Exactly. And that does change over time. We're like, oh, we're asking the wrong question. Actually, there's a more fundamental question, or there's two questions here that we've piled into one.
[ 0:51:09.000 ---> 0:51:58.000 ] Speaker A : Yeah, I wanted to ask something about what you call the impact and truth scores. And I wanted to ask if you have some weights in the positions or in the arguments of some sort. And I wanted to note that in the current interface, because it shows the amount of quotes that each argument has, it can imply that some arguments are more popular than others and have more weight than others. And so as a user, you could say, oh, I'm only going to explore positions and arguments that have a lot of claims because I think these are the most relevant ones. So that's the kind of waiting that's unconsciously happening there. And if you had any plans to deal with that, or more specifically or not.
[ 0:51:58.000 ---> 0:54:36.000 ] Speaker B : Basically, I'll start with your latter question, then answer the question you said before. So one first of all, thanks. Great feedback. Like super great feedback. Again, I want to stress we're a tiny nonprofit. I would love to have a very large budget to have a design firm and do lots of user testing because literally, it's not just about the data itself that's going to improve people's relationship to information. Because we're visual creatures, we have all these heuristics and biases. So we do have to figure out all the different visual cues that will make it so people are seeing the information a little bit more rationally. The meaning is coming through without any extra or without losing anything. So there's a lot of work left to do those like badges and quotes and then things like that. It's just like how far we've gotten now and the features that we find important, which is essentially chain of providence. The quotes are chain of providence. If the evidence is a study in a 478 page report, I find it to be ridiculous as a linked knowledge standard for someone to make a claim and be like, here's a 400 page article. Are you kidding me? So the quote feature is about taking that paragraph context of where that came from and saying the claim came from this here, and then you can go that next layer and go look at the original artifact if you want. So thanks for that feedback. We're always going to be working on iterating more and making it less biased in terms of its visual representation. And on papers, we don't have that right. On papers, you don't see how many quotes there are per node until you unpack it and see it. So that kind of removes that as a front end. And then for the impacts and truth scores, which we need to change immediately. Again, those were like the idea of the person create the technologist who created Debate Map, who's a partner of ours. We do not have any metrics. We do not have any weighting. There are different dimensions of how those scores would be implemented. So people can vote on things that is in the infrastructure of the technology, but we have turned it off in the Society. We call it the society library standard. The creator of Debatemap has maintained the use of that functionality. The Society Library has turned it off even to see we don't even want users to see it or think about it, but the capability within the technology is there if we want to iterate upon that and use it later. But that is a huge, I said, can of worms because that could be implying so much subjective bias in the structure of that that we would have to undertake immense amount of labor to figure out how to do that. Right. And we're not there yet. So it's a technological capability. We have turned it off and we have no interest in using it anytime soon.
[ 0:54:36.000 ---> 0:54:44.000 ] Speaker A : Good. Awesome. Do the researchers also see it or not? Do the people that are doing the.
[ 0:54:44.000 ---> 0:54:46.000 ] Speaker B : Society Library standard thank you.
[ 0:54:46.000 ---> 0:54:47.000 ] Speaker A : Thank you.
[ 0:54:48.000 ---> 0:54:58.000 ] Speaker B : Good should remove it from the presentation because it's misleading. Yes, it's the capability of the technology, but we never use it, and we explicitly removed it from our own standard.
[ 0:54:58.000 ---> 0:55:08.000 ] Speaker A : It's not just that. It's probably the biggest fear that people have about something like this, which is you will influence public opinion if you have an agenda and if you blah, blah, blah, blah.
[ 0:55:08.000 ---> 0:55:08.000 ] Speaker B : Right.
[ 0:55:08.000 ---> 0:55:11.000 ] Speaker A : So if you want to remain neutral, you have to disable it.
[ 0:55:12.000 ---> 0:55:50.000 ] Speaker B : Yes. The creator of Debate Map. His name is Venrix. And when I mentioned we looked at 168 different debate mapping systems to see what pitfalls they fell into. I found his work and I was like, yours is my favorite and the best I've ever seen. And he's like, well, I love the mission of the Society Library. I'll team up. And he's been adapting his technology for our use case for like four years now, which is very nice of him. So he's a part of the team, but he keeps his features that he likes. And we're like, okay, just it's not a part of the Society Library standard. You keep that over there. Society Library is like, we're going to turn some things off that you good boundary.
[ 0:55:50.000 ---> 0:55:51.000 ] Speaker A : Good boundary. Yeah.
[ 0:55:54.000 ---> 0:55:56.000 ] Speaker B : Any other questions?
[ 0:56:00.000 ---> 0:56:07.000 ] Speaker A : I have more questions. If nobody steps up, is the training that you do open source?
[ 0:56:08.000 ---> 0:57:36.000 ] Speaker B : It will be. So we're going to be taking a lot of our methods and bring it to the fact checkers. And the fact checking thing is going to be open source. What I do want to do is moocify what we've done and make that open source because especially we teach students this methodology and we teach analysts this methodology, but we keep an. Eye, it's very much observed. So we see where people are struggling and we're like, okay, we'll give you another exercise. So because we're training people with the intent of them being good, independent sovereign workers for us and for the mission, we're really invested. We've got the exercises, we've got the documents. This is what the Society Library considers a claim and blah, blah, blah. We have all the documentation, but we're really hands on when it comes to the training and development of people who are going to be touching the database. So I think it's going to take a little bit of thought to think about how you can mukify that and be like, what's the best system out there where if someone fails a test, it's going to give them another exercise. So people don't just go through the thing and then think they've mastered the skill, but make it so that they really master the skill. Because normally it's my COO and I that pay very close attention to that and make sure it happens, or just make sure, like, okay, they're not getting this dimension of the work. So we can't give them that responsibility. We got to cut that out of their responsibilities. And that's all very hands on. So it's difficult to scale, but we do want to scale it once we have more time to actually dig into. How do you produce educational curricula that's not observed by any teacher?
[ 0:57:39.000 ---> 0:57:40.000 ] Speaker A : Cool. Thank you. That would be awesome.
[ 0:57:42.000 ---> 0:58:50.000 ] Speaker B : One thing I will say is that some of the techniques that we took ideas from was the 2009, which is outdated wildly now. 2009. CIA, trader primecraft. Primer tradecraft. Yeah. Tradecraft primer. That's it. If you look up that intelligence agents in the United States have to be very unbiased, as unbiased as you could imagine. The consequences of their intelligent work matters greatly. And so that's where we got a lot of our initial ideas about devil's advocacy research and stuff like that. We just went to see what was publicly available about intelligence agents and what are their methods and tools and tricks, and then we started implementing it. And then again, it all depends on what your research outcome is. We develop the tools and methods because we're trying to get different outcome. The sciences don't share all the same methods and tools to make their observation. Depending upon what you're trying to observe, the conclusion you're trying to come to, you have to use an electron microscope or a blood test. There's all these different tests and measures and tools and things. So depending upon what you're doing, there are bespoke things for you. And my laptop's about to die, so let me grab my charger.
[ 0:58:51.000 ---> 0:59:21.000 ] Speaker A : No worries. In the meantime, anybody else has questions for Jamie and this wonderfully in depth presentation that she gave about the process, which is super nice. So think about your questions. And we are seven minutes past the hour or six minutes past the hour, and we're already late. But the topic is interesting, so if people are still here, we'll continue to answer people's questions. So anybody has any question for Jamie.
[ 0:59:24.000 ---> 0:59:49.000 ] Speaker C : I would ask a question. So per se. So now it's very limited, right, on the information it has. It's just basically somebody puts so do I have an access as me personally go there and just start kind of framing it, asking per se solution, and then go towards the question or I don't have an access myself.
[ 0:59:49.000 ---> 1:00:46.000 ] Speaker B : So that map that I showed you and that paper that I showed you, you have access to that on Societylibrary.org right now. You just go to the nuclear energy section, go to the state level issue. There's the Diablo Canyon stuff. We have not published climate change, COVID election integrity stuff because it's just so wildly incomplete. We only got to the main topic flyover stage in some mass gathering of claims. Too much work to get done. Collections are expensive. Now that we've completed and published our first collection, now we're focused on, okay, how much of this can we automate, and how do we get the price per claim input down so that it's much more feasible to tackle societal scale issues? Yeah, so there's very much a fledgling organization trying to do something that people have not succeeded at for 30 years in the specific domain. Obviously, there's wildly successful knowledge management companies and things like that, but long way to go.
[ 1:00:46.000 ---> 1:00:49.000 ] Speaker C : It was so needed. I can see this.
[ 1:00:50.000 ---> 1:00:54.000 ] Speaker B : It's amazing. I love it. Yay. Thank you.
[ 1:00:54.000 ---> 1:01:36.000 ] Speaker A : And Jamie is too polite to say this, but I will say it. Everybody can donate to the Society library to fund their efforts, and so that's also one of the ways to support it. And I think the world would become a better place if we so I think we should attempt to do that. And if there's no more questions, let's wrap this up. Thank you so much, Jamie, for your time, for your energy, for your explanations, and so much detail that you went through. We will have more events coming soon, and so you can follow now and join the discord and all those things that you're already probably aware of. And thank you so much.
[ 1:01:36.000 ---> 1:01:39.000 ] Speaker B : Yeah, of course. Thanks for having me. It's a pleasure.
[ 1:01:39.000 ---> 1:01:40.000 ] Speaker A : Bye bye. Thank you.
[ 1:01:40.000 ---> 1:01:47.000 ] Speaker B : Thank you, Jamie. Of course. Cheers. Way.
